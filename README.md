# k8s setup in ubuntu


# ë¦¬ëˆ…ìŠ¤ì— kubectl ì„¤ì¹˜ ë° ì„¤ì •
#    https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
##    ë¦¬ëˆ…ìŠ¤ì— kubectl ì„¤ì¹˜
        ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ë¦¬ëˆ…ìŠ¤ì— kubectlì„ ì„¤ì¹˜í•  ìˆ˜ ìˆë‹¤.

        ë¦¬ëˆ…ìŠ¤ì— curlì„ ì‚¬ìš©í•˜ì—¬ kubectl ë°”ì´ë„ˆë¦¬ ì„¤ì¹˜
        ê¸°ë³¸ íŒ¨í‚¤ì§€ ê´€ë¦¬ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì¹˜
        ë‹¤ë¥¸ íŒ¨í‚¤ì§€ ê´€ë¦¬ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì¹˜


## ë¦¬ëˆ…ìŠ¤ì—ì„œ curlì„ ì‚¬ìš©í•˜ì—¬ kubectl ë°”ì´ë„ˆë¦¬ ì„¤ì¹˜ 
    1. ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ìµœì‹  ë¦´ë¦¬ìŠ¤ë¥¼ ë‹¤ìš´ë¡œë“œí•œë‹¤.
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

    2. ë°”ì´ë„ˆë¦¬ë¥¼ ê²€ì¦í•œë‹¤. (ì„ íƒ ì‚¬í•­)
        kubectl ì²´í¬ì„¬(checksum) íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•œë‹¤.
            curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"

        kubectl ë°”ì´ë„ˆë¦¬ë¥¼ ì²´í¬ì„¬ íŒŒì¼ì„ í†µí•´ ê²€ì¦í•œë‹¤.
            echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
                ê²€ì¦ì´ ì„±ê³µí•œë‹¤ë©´, ì¶œë ¥ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
                    kubectl: OK
    3. kubectl ì„¤ì¹˜
        sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

    4. ì„¤ì¹˜í•œ ë²„ì „ì´ ìµœì‹ ì¸ì§€ í™•ì¸í•œë‹¤.
        kubectl version --client


## kubectl êµ¬ì„± í™•ì¸ 
    kubectl cluster-info



        kubectl cluster-info
        URL ì‘ë‹µì´ í‘œì‹œë˜ë©´, kubectlì´ í´ëŸ¬ìŠ¤í„°ì— ì ‘ê·¼í•˜ë„ë¡ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ëœ ê²ƒì´ë‹¤.

        ë‹¤ìŒê³¼ ë¹„ìŠ·í•œ ë©”ì‹œì§€ê°€ í‘œì‹œë˜ë©´, kubectlì´ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„°ì— ì—°ê²°í•  ìˆ˜ ì—†ë‹¤.

        The connection to the server <server-name:port> was refused - did you specify the right host or port?
        ì˜ˆë¥¼ ë“¤ì–´, ë©í†±ì—ì„œ ë¡œì»¬ë¡œ ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„°ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´, Minikubeì™€ ê°™ì€ ë„êµ¬ë¥¼ ë¨¼ì € ì„¤ì¹˜í•œ ë‹¤ìŒ ìœ„ì—ì„œ ì–¸ê¸‰í•œ ëª…ë ¹ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì•¼ í•œë‹¤.

        kubectl cluster-infoê°€ URL ì‘ë‹µì„ ë°˜í™˜í•˜ì§€ë§Œ í´ëŸ¬ìŠ¤í„°ì— ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´ ë‹¤ìŒì„ ì‚¬ìš©í•œë‹¤.

        kubectl cluster-info dump




        IN Windows PowerShell

            PS C:\Users\sangbinlee9> ssh sangbinlee9@192.168.0.8 -p 22
            sangbinlee9@master:~$ sudo su
            root@master:/home/sangbinlee9# minikube start --force
            ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

            root@master:/home/sangbinlee9# kubectl get pods --all-namespaces -o wide
            NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
            kube-system   coredns-787d4945fb-6wc2q           1/1     Running   0          12s   10.244.0.2     minikube   <none>           <none>
            kube-system   etcd-minikube                      1/1     Running   0          26s   192.168.49.2   minikube   <none>           <none>
            kube-system   kube-apiserver-minikube            1/1     Running   0          26s   192.168.49.2   minikube   <none>           <none>
            kube-system   kube-controller-manager-minikube   1/1     Running   0          27s   192.168.49.2   minikube   <none>           <none>
            kube-system   kube-proxy-jg6n7                   1/1     Running   0          12s   192.168.49.2   minikube   <none>           <none>
            kube-system   kube-scheduler-minikube            1/1     Running   0          24s   192.168.49.2   minikube   <none>           <none>
            kube-system   storage-provisioner                1/1     Running   0          23s   192.168.49.2   minikube   <none>           <none>
            root@master:/home/sangbinlee9# kubectl cluster-info
            Kubernetes control plane is running at https://192.168.49.2:8443
            CoreDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

            To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
            
            root@master:/home/sangbinlee9#





            root@master:/home/sangbinlee9#  kubectl cluster-info
            Kubernetes control plane is running at https://192.168.49.2:8443
            CoreDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

            To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
            root@master:/home/sangbinlee9#

# í˜„ì¬ ì‚¬ìš©í•˜ê³  ìˆëŠ” ì‰˜ í™•ì¸í•˜ê¸°

    root@master:~# grep root /etc/passwd
    root:x:0:0:root:/root:/bin/bash
    root@master:~#










## ë¦¬ëˆ…ìŠ¤ì—ì„œ curlì„ ì‚¬ìš©í•˜ì—¬ kubectl ë°”ì´ë„ˆë¦¬ ì„¤ì¹˜ 
## ë¦¬ëˆ…ìŠ¤ì—ì„œ curlì„ ì‚¬ìš©í•˜ì—¬ kubectl ë°”ì´ë„ˆë¦¬ ì„¤ì¹˜ 
## ë¦¬ëˆ…ìŠ¤ì—ì„œ curlì„ ì‚¬ìš©í•˜ì—¬ kubectl ë°”ì´ë„ˆë¦¬ ì„¤ì¹˜ 
    ì„¤ì¹˜ìˆœì„œ
    âœ”  ê³µí†µ ì„¤ì¹˜(master node, worker node ì— ì„¤ì¹˜)

    1) CRI ì„¤ì¹˜ - ì»¨í…Œì´ë„ˆ ëŸ°íƒ€ì„

    2) kubeadm, kubectl, kubelet ì„¤ì¹˜

    3) CNI ì„¤ì¹˜

    

    âœ”  master node

    - kubeadm init ì‹¤í–‰

    

    âœ”  worker node

    - kubeadm join ì‹¤í–‰

    





















# master, worker1, worker2



    ## í•„ìˆ˜ í¬íŠ¸ í™•ì¸ - ì»´í“¨í„°ì˜ íŠ¹ì • í¬íŠ¸ë“¤ ê°œë°©.

        í¬íŠ¸ì™€ í”„ë¡œí† ì½œ
            ë¬¼ë¦¬ì  ë„¤íŠ¸ì›Œí¬ ë°©í™”ë²½ì´ ìˆëŠ” ì˜¨í”„ë ˆë¯¸ìŠ¤ ë°ì´í„° ì„¼í„° ë˜ëŠ” í¼ë¸”ë¦­ í´ë¼ìš°ë“œì˜ ê°€ìƒ ë„¤íŠ¸ì›Œí¬ì™€ ê°™ì´ ë„¤íŠ¸ì›Œí¬ ê²½ê³„ê°€ ì—„ê²©í•œ í™˜ê²½ì—ì„œ ì¿ ë²„ë„¤í‹°ìŠ¤ë¥¼ ì‹¤í–‰í•  ë•Œ, ì¿ ë²„ë„¤í‹°ìŠ¤ êµ¬ì„± ìš”ì†Œì—ì„œ ì‚¬ìš©í•˜ëŠ” í¬íŠ¸ì™€ í”„ë¡œí† ì½œì„ ì•Œê³  ìˆëŠ” ê²ƒì´ ìœ ìš©í•˜ë‹¤.

        ì»¨íŠ¸ë¡¤ í”Œë ˆì¸
            í”„ë¡œí† ì½œ	ë°©í–¥	í¬íŠ¸ ë²”ìœ„	ìš©ë„	ì‚¬ìš© ì£¼ì²´
            TCP	ì¸ë°”ìš´ë“œ	6443	ì¿ ë²„ë„¤í‹°ìŠ¤ API ì„œë²„	ì „ë¶€
            TCP	ì¸ë°”ìš´ë“œ	2379-2380	etcd ì„œë²„ í´ë¼ì´ì–¸íŠ¸ API	kube-apiserver, etcd
            TCP	ì¸ë°”ìš´ë“œ	10250	Kubelet API	Self, ì»¨íŠ¸ë¡¤ í”Œë ˆì¸
            TCP	ì¸ë°”ìš´ë“œ	10259	kube-scheduler	Self
            TCP	ì¸ë°”ìš´ë“œ	10257	kube-controller-manager	Self
            etcd í¬íŠ¸ê°€ ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ ì„¹ì…˜ì— í¬í•¨ë˜ì–´ ìˆì§€ë§Œ, ì™¸ë¶€ ë˜ëŠ” ì‚¬ìš©ì ì§€ì • í¬íŠ¸ì—ì„œ ìì²´ etcd í´ëŸ¬ìŠ¤í„°ë¥¼ í˜¸ìŠ¤íŒ…í•  ìˆ˜ë„ ìˆë‹¤.

        ì›Œì»¤ ë…¸ë“œ
            í”„ë¡œí† ì½œ	ë°©í–¥	í¬íŠ¸ ë²”ìœ„	ìš©ë„	ì‚¬ìš© ì£¼ì²´
            TCP	ì¸ë°”ìš´ë“œ	10250	Kubelet API	Self, ì»¨íŠ¸ë¡¤ í”Œë ˆì¸
            TCP	ì¸ë°”ìš´ë“œ	30000-32767	NodePort ì„œë¹„ìŠ¤â€ 	ì „ë¶€

        # Master
            sudo ufw enable
            sudo ufw allow 6443/tcp
            sudo ufw allow 2379:2380/tcp
            sudo ufw allow 10250/tcp
            sudo ufw allow 10259/tcp
            sudo ufw allow 10257/tcp
            sudo ufw status

        # Worker
            sudo ufw enable
            sudo ufw allow 10250/tcp
            sudo ufw allow 30000:32767/tcp
            sudo ufw status
            ## IPv4ë¥¼ í¬ì›Œë”©í•˜ì—¬ iptablesê°€ ë¸Œë¦¬ì§€ëœ íŠ¸ë˜í”½ì„ ë³´ê²Œ í•˜ê¸°



    # Swap off

        sudo swapoff -a && sudo sed -i '/swap/s/^/#/' /etc/fstab




# ì»¨í…Œì´ë„ˆ ëŸ°íƒ€ì„

    # íŒŒë“œê°€ ë…¸ë“œì—ì„œ ì‹¤í–‰ë  ìˆ˜ ìˆë„ë¡ í´ëŸ¬ìŠ¤í„°ì˜ ê° ë…¸ë“œì— ì»¨í…Œì´ë„ˆ ëŸ°íƒ€ì„ì„ ì„¤ì¹˜í•´ì•¼ í•œë‹¤. 


        ### Installing containerd
            $ tar Cxzvf /usr/local containerd-1.6.2-linux-amd64.tar.gz






            cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
            overlay
            br_netfilter
            EOF

            sudo modprobe overlay
            sudo modprobe br_netfilter

            # í•„ìš”í•œ sysctl íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ë©´, ì¬ë¶€íŒ… í›„ì—ë„ ê°’ì´ ìœ ì§€ëœë‹¤.
            cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
            net.bridge.bridge-nf-call-iptables  = 1
            net.bridge.bridge-nf-call-ip6tables = 1
            net.ipv4.ip_forward                 = 1
            EOF

            # ì¬ë¶€íŒ…í•˜ì§€ ì•Šê³  sysctl íŒŒë¼ë¯¸í„° ì ìš©í•˜ê¸°
            sudo sysctl --system





# master
    sudo kubeadm init


        Your Kubernetes control-plane has initialized successfully!

        To start using your cluster, you need to run the following as a regular user:

        mkdir -p $HOME/.kube
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config

        Alternatively, if you are the root user, you can run:

        export KUBECONFIG=/etc/kubernetes/admin.conf

        You should now deploy a pod network to the cluster.
        Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
        https://kubernetes.io/docs/concepts/cluster-administration/addons/

        Then you can join any number of worker nodes by running the following on each as root:

        kubeadm join 192.168.0.8:6443 --token g0w2aj.90u5ctdqjvw9wzuc \
                --discovery-token-ca-cert-hash sha256:ba53a230869d5abfa5d023ebfbbe19557cf984ec978f49468bbb758a61bebea5



# worker1, worker2
        kubeadm join 192.168.0.8:6443 --token g0w2aj.90u5ctdqjvw9wzuc \
                --discovery-token-ca-cert-hash sha256:ba53a230869d5abfa5d023ebfbbe19557cf984ec978f49468bbb758a61bebea5





# sangbinlee9@master:~$  kubeadm reset






    root@master:~# kubeadm init
    [init] Using Kubernetes version: v1.27.2
    [preflight] Running pre-flight checks
    [preflight] Pulling images required for setting up a Kubernetes cluster
    [preflight] This might take a minute or two, depending on the speed of your internet connection
    [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
    W0520 08:22:20.887915    3221 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.2, falling back to the nearest etcd version (3.5.7-0)
    W0520 08:22:20.996404    3221 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.8" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
    [certs] Using certificateDir folder "/etc/kubernetes/pki"
    [certs] Generating "ca" certificate and key
    [certs] Generating "apiserver" certificate and key
    [certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master] and IPs [10.96.0.1 192.168.0.8]
    [certs] Generating "apiserver-kubelet-client" certificate and key
    [certs] Generating "front-proxy-ca" certificate and key
    [certs] Generating "front-proxy-client" certificate and key
    [certs] Generating "etcd/ca" certificate and key
    [certs] Generating "etcd/server" certificate and key
    [certs] etcd/server serving cert is signed for DNS names [localhost master] and IPs [192.168.0.8 127.0.0.1 ::1]
    [certs] Generating "etcd/peer" certificate and key
    [certs] etcd/peer serving cert is signed for DNS names [localhost master] and IPs [192.168.0.8 127.0.0.1 ::1]
    [certs] Generating "etcd/healthcheck-client" certificate and key
    [certs] Generating "apiserver-etcd-client" certificate and key
    [certs] Generating "sa" key and public key
    [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
    [kubeconfig] Writing "admin.conf" kubeconfig file
    [kubeconfig] Writing "kubelet.conf" kubeconfig file
    [kubeconfig] Writing "controller-manager.conf" kubeconfig file
    [kubeconfig] Writing "scheduler.conf" kubeconfig file
    [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
    [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
    [kubelet-start] Starting the kubelet
    [control-plane] Using manifest folder "/etc/kubernetes/manifests"
    [control-plane] Creating static Pod manifest for "kube-apiserver"
    [control-plane] Creating static Pod manifest for "kube-controller-manager"
    [control-plane] Creating static Pod manifest for "kube-scheduler"
    [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
    W0520 08:22:24.124819    3221 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.2, falling back to the nearest etcd version (3.5.7-0)
    [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
    [apiclient] All control plane components are healthy after 5.506057 seconds
    [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
    [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
    [upload-certs] Skipping phase. Please see --upload-certs
    [mark-control-plane] Marking the node master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
    [mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
    [bootstrap-token] Using token: r3zhpf.wwfpbhenjnerkqlg
    [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
    [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
    [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
    [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
    [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
    [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
    [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
    [addons] Applied essential addon: CoreDNS
    [addons] Applied essential addon: kube-proxy

    Your Kubernetes control-plane has initialized successfully!

    To start using your cluster, you need to run the following as a regular user:

    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

    Alternatively, if you are the root user, you can run:

    export KUBECONFIG=/etc/kubernetes/admin.conf

    You should now deploy a pod network to the cluster.
    Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
    https://kubernetes.io/docs/concepts/cluster-administration/addons/

    Then you can join any number of worker nodes by running the following on each as root:

    kubeadm join 192.168.0.8:6443 --token r3zhpf.wwfpbhenjnerkqlg \
            --discovery-token-ca-cert-hash sha256:fabf5cec4f7e24fb881de176c1b03c88c7b7aedc246636f587f47081f82f5787
    root@master:~#


kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml


